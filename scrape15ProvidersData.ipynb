{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script scrapes 15 providers data from Reed, from which daily revenue and market share will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic and advanced modules\n",
    "from IPython.core.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nest_asyncio\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Any providers\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def scrape(url):\n",
    "    start = time.time()\n",
    "    \n",
    "    # Initialize variables to be extracted\n",
    "    link = []\n",
    "    title = []\n",
    "    provider = []\n",
    "    subtitle = []\n",
    "    unit_sold = []\n",
    "    offer_price = []\n",
    "    ori_price = []\n",
    "    saving = []\n",
    "    sold_or_enq = []\n",
    "    \n",
    "    \n",
    "    # Get the no of pages to scrape\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    lst_stop_page = soup.find('span',class_='h1').text.split() # Gets total no of course as list of strings\n",
    "    str_stop_page = ''.join(lst_stop_page) # converts list into string\n",
    "    str_stop_page = str_stop_page.replace(',','') # removes comma from the string\n",
    "    stop_page = np.int(str_stop_page) # Converts str into int\n",
    "    stop_page = np.ceil(stop_page/100) # Returns float\n",
    "    stop_page = np.int(stop_page) # Converts back to int from float required for range function\n",
    "    \n",
    "    \n",
    "    # urls parsing\n",
    "    async def fetch(session, url):\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "        \n",
    "    async def main():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for page in range(1,stop_page+1):\n",
    "                html = await fetch(session, url + f'?pageno={page}&sortby=MostPopular&pagesize=100')\n",
    "                soup = BeautifulSoup(html,'html.parser')\n",
    "                for lnk in soup.find_all('h2',class_=\"mt-4 mt-sm-1 mr-5 mb-0\"):\n",
    "                    link.append(str('https://www.reed.co.uk')+lnk.find('a').get('href'))\n",
    "                    \n",
    "    asyncio.run(main())\n",
    "    \n",
    "                    \n",
    "    # Information parsing\n",
    "    async def fetch(session, url):\n",
    "        async with session.get(url) as response:\n",
    "            return await response.text()\n",
    "        \n",
    "    async def main():\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            # Count the no of requests\n",
    "            req = 0\n",
    "            for lnk,req_count in zip(link,range(1,len(link)+1)):\n",
    "                html = await fetch(session, lnk)\n",
    "                req = req+1\n",
    "                print(f'Requests Completed: {req} out of {len(link)}')\n",
    "                soup = BeautifulSoup(html,'html.parser')\n",
    "                # Clear all the outputs except the current one in notebook console\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                # Extract title\n",
    "                title_tag = soup.find('h1')\n",
    "                title.append(title_tag.text if title_tag is not None else 'missing')\n",
    "                \n",
    "                #Extract subtitle\n",
    "                subtitle_tag = soup.find('h2')\n",
    "                subtitle.append(subtitle_tag.text if subtitle_tag is not None else 'missing')\n",
    "                \n",
    "                # Extract offer price\n",
    "                offer_price_tag = soup.find('span',class_='current-price')\n",
    "                offer_price.append(offer_price_tag.text if offer_price_tag is not None else 'missing')\n",
    "                \n",
    "                # Extract original price\n",
    "                ori_price_tag = soup.find(\"small\",class_='vat-status')\n",
    "                ori_price.append(ori_price_tag.text if ori_price_tag is not None else 'missing')\n",
    "                \n",
    "                # Extract units sold\n",
    "                unit_sold_tag = soup.find_all('strong')[1]\n",
    "                unit_sold.append(unit_sold_tag.text if unit_sold_tag is not None else 'missing')\n",
    "                \n",
    "                # Extract providers\n",
    "                provider_tag = soup.find('section',class_='sidebar-actions').find('a',class_='provider-link')\n",
    "                provider.append(provider_tag.text if provider_tag is not None else 'missing')\n",
    "                \n",
    "                # Extract savings\n",
    "                saving_tag = soup.find(\"span\",class_=\"icon-savings-tag price-saving\")\n",
    "                saving.append(saving_tag.text if saving_tag is not None else 'missing')\n",
    "                \n",
    "                # Extract it the course is sold or enquired\n",
    "                sold_er_enq_tag = soup.find_all(\"div\",class_=\"summary-content\")[-1]\n",
    "                sold_or_enq.append(sold_er_enq_tag.text.strip() if sold_er_enq_tag is not None else 'missing')\n",
    "                \n",
    "    asyncio.run(main())\n",
    "    \n",
    "    #Create a df of extracted variables\n",
    "    df = pd.DataFrame({'title':title,'link':link, 'provider':provider, 'subtitle':subtitle,\n",
    "                       'price':offer_price,'original_price':ori_price,'sold':unit_sold,'saving':saving,'sold_or_enq':sold_or_enq})\n",
    "    duration = np.round((time.time()-start)/60,2)\n",
    "    \n",
    "    # Get course ids\n",
    "    df['id'] = df.link.str.split('/').str[5].str.replace('#','')\n",
    "    \n",
    "    # Clean original price\n",
    "    actual_price = []\n",
    "    for price in df.original_price:\n",
    "        actual_price.append(price[15:-1])\n",
    "        \n",
    "    # Rewrite original_price\n",
    "    df['original_price'] = actual_price\n",
    "    \n",
    "    # Clean price. Remove £ and comma(,). And convert to float\n",
    "    \"Remove comma before applying pd.to_numeric(). Otherwise values with comma will be converted to zero\"\n",
    "    df.price = df.price.str.replace('£','').str.replace(',','')\n",
    "    df.price = pd.to_numeric(df.price, errors='coerce').fillna(0).astype(float)\n",
    "    \n",
    "    # Convert non-digit sold to na, fill na by 0 and cast to int\n",
    "    \"Remove comma before applying pd.to_numeric(). Otherwise values with comma will be converted to zero\"\n",
    "    df.sold = df.sold.str.replace(',','')\n",
    "    df.sold = pd.to_numeric(df.sold,errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    ## Drop all the duplicates for precautionary measure\n",
    "    df.drop_duplicates(subset=['id'],keep=False,inplace=True)\n",
    "    \n",
    "    print(f'{df.provider.iloc[0]} Courses: Time required to scrape {len(df)} observation: {duration} minutes')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centre of Excellence Courses: Time required to scrape 347 observation: 3.12 minutes\n"
     ]
    }
   ],
   "source": [
    "## One education\n",
    "oneEducation = scrape('https://www.reed.co.uk/courses/one-education/p1812')\n",
    "\n",
    "## Course Gate\n",
    "courseGate = scrape('https://www.reed.co.uk/courses/course-gate/p1834')\n",
    "\n",
    "## Janets\n",
    "janets = scrape('https://www.reed.co.uk/courses/janets/p1778')\n",
    "\n",
    "## Euston college\n",
    "eustonCollege = scrape('https://www.reed.co.uk/courses/euston-college/p2128')\n",
    "\n",
    "# Training Express\n",
    "trainingExpress = scrape('https://www.reed.co.uk/courses/training-express-ltd/p2079')\n",
    "\n",
    "## Beaco\n",
    "beaco = scrape('https://www.reed.co.uk/courses/be-acouk/p545')\n",
    "\n",
    "## Brentwood\n",
    "brentwood = scrape('https://www.reed.co.uk/courses/brentwood-open-learning-college/p438')\n",
    "\n",
    "## Oplex\n",
    "oplexCareers = scrape('https://www.reed.co.uk/courses/oplex-careers/p630')\n",
    "\n",
    "## Oxford\n",
    "oxford = scrape('https://www.reed.co.uk/courses/oxford-home-study-college/p1245')\n",
    "\n",
    "## CPD courses\n",
    "cpdCourses = scrape('https://www.reed.co.uk/courses/cpd-courses/p1534')\n",
    "\n",
    "## Ofcourse\n",
    "ofCourse = scrape('https://www.reed.co.uk/courses/ofcourse/p675')\n",
    "\n",
    "## Training terminal\n",
    "trainingTerminal = scrape('https://www.reed.co.uk/courses/the-training-terminal/p1064')\n",
    "\n",
    "## Tremdimi\n",
    "trendimi = scrape('https://www.reed.co.uk/courses/trendimi/p964')\n",
    "\n",
    "## Excel with business\n",
    "excelWithBusiness = scrape('https://www.reed.co.uk/courses/excel-with-business/p930')\n",
    "\n",
    "## Centre of Excellence\n",
    "centreOfExcellence = scrape('https://www.reed.co.uk/courses/centre-of-excellence-online/p652')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the providers data into one dataframe and save as csv\n",
    "mergedDf = pd.concat([\n",
    "    oneEducation,\n",
    "    courseGate,\n",
    "    janets,\n",
    "    eustonCollege,\n",
    "    trainingExpress,\n",
    "    beaco,\n",
    "    brentwood,\n",
    "    oplexCareers,\n",
    "    oxford,\n",
    "    cpdCourses,\n",
    "    ofCourse,\n",
    "    trainingTerminal,\n",
    "    trendimi,\n",
    "    excelWithBusiness,\n",
    "    centreOfExcellence\n",
    "    \n",
    "],axis=0).reset_index(drop=True)\n",
    "\n",
    "# Today's date to save as csv\n",
    "today = datetime.today().strftime('%d_%b')\n",
    "mergedDf.to_csv(f\"{today}_15_providers.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
